{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "373bf2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0358c152",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(NN_data_path: str, output_var: str, train_prop: float, val_prop: float, test_prop: float):\n",
    "    \"\"\"\n",
    "    Prepares the input and output data for a neural network model in Tensorflow.\n",
    "    The operations performed to the data are:\n",
    "    dropping the identifier column from the input data,\n",
    "    splitting the data into training, validation, and test sets,\n",
    "    scaling the features into a closed range of [0, 1],\n",
    "    converting  the data into 'tf.tensors',\n",
    "    adding an additional dimension into the input tensors so that they can be fed into convolutional layers.\n",
    "    \n",
    "    Args:\n",
    "    NN_data_path -- folder where the input and output data are located\n",
    "    output_var -- name of the output variable that you want data from\n",
    "    train_prop -- proportion of the data to be used for training\n",
    "    val_prop -- proportion of the data to be used for validation\n",
    "    test_prop -- proportion of the data to be used for testing\n",
    "    \n",
    "    Returns:\n",
    "    data_dict -- a dict containing the following data:\n",
    "                 scaled_X_train,\n",
    "                 scaled_X_val,\n",
    "                 scaled_X_test,\n",
    "                 Y_train_output_var,\n",
    "                 Y_val_output_var,\n",
    "                 Y_test_output_var.             \n",
    "                 \n",
    "    Notes:\n",
    "    - train_prop + val_prop + test_prop must be equal to 1.\n",
    "    - options for output_var include: tdir_down, tdif_down, tdir_up, tdif_up, spherical_albedo, edir, edif, path_rad.\n",
    "    \"\"\"\n",
    "    output_var = output_var.lower()\n",
    "    assert train_prop + val_prop + test_prop == 1, \"train_prop + val_prop + test_prop must be equal to 1.\"\n",
    "    assert output_var in [\"tdir_down\",\"tdif_down\",\"tdir_up\",\"tdif_up\",\"spherical_albedo\",\"edir\",\"edif\",\"path_rad\"], \"output not found.\"\n",
    "    \n",
    "    # Inputs\n",
    "    X = pd.read_csv(NN_data_path + \"inputs.csv\", index_col=0)\n",
    "    X = X.drop(\"atmosphere_file\", axis=1)    # Drop the identifier col\n",
    "    \n",
    "    def load_HDF5(folder_path: str, file_name: str):\n",
    "        with h5py.File(name=folder_path+file_name, mode=\"r\") as hf:\n",
    "            data = hf[\"output_data\"][:]\n",
    "        return data\n",
    "    \n",
    "    # Outputs\n",
    "    Y = load_HDF5(NN_data_path, \"outputs_subset.h5\")\n",
    "    \n",
    "    ###### Calculate the train, validation, and test sizes #####\n",
    "    n = X.shape[0]    # number of samples\n",
    "    num_train_samples = int(n * train_prop)\n",
    "    num_val_samples = int(n * val_prop)\n",
    "    num_test_samples = int(n - num_train_samples - num_val_samples)\n",
    "    \n",
    "    ###### Data splits ######\n",
    "    # (X, Y) ==> (train | temp)\n",
    "    # temp ==> (validation | test)\n",
    "    X_train, X_temp, Y_train, Y_temp = train_test_split(X, Y, train_size=num_train_samples, random_state=42)\n",
    "    X_val, X_test, Y_val, Y_test = train_test_split(X_temp, Y_temp, test_size=num_test_samples, random_state=42)\n",
    "    \n",
    "    \n",
    "    ##### Scale the features for numerical stability #####\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaler.fit(X_train)\n",
    "    scaled_X_train = scaler.transform(X_train)\n",
    "    scaled_X_val = scaler.transform(X_val)\n",
    "    scaled_X_test = scaler.transform(X_test)\n",
    "    \n",
    "    ##### Convert the data to tf.tensors #####\n",
    "    scaled_X_train = tf.cast(scaled_X_train, dtype=tf.float32)\n",
    "    scaled_X_val = tf.cast(scaled_X_val, dtype=tf.float32)\n",
    "    scaled_X_test = tf.cast(scaled_X_test, dtype=tf.float32)\n",
    "    Y_train = tf.cast(Y_train, dtype=tf.float32)\n",
    "    Y_val = tf.cast(Y_val, dtype=tf.float32)\n",
    "    Y_test = tf.cast(Y_test, dtype=tf.float32)\n",
    "    \n",
    "    ##### Add an extra dimension to the input data for convolution #####\n",
    "    scaled_X_train = tf.expand_dims(scaled_X_train, axis=-1)\n",
    "    scaled_X_val = tf.expand_dims(scaled_X_val, axis=-1)\n",
    "    scaled_X_test = tf.expand_dims(scaled_X_test, axis=-1)\n",
    "    \n",
    "    ##### Create the 8 different outputs #####\n",
    "    output_cols_in_order = [\"tdir_down\",\"tdif_down\",\"tdir_up\",\"tdif_up\",\"spherical_albedo\",\"edir\",\"edif\",\"path_rad\"]\n",
    "    output_col_indices = {col_name: idx for idx, col_name in enumerate(output_cols_in_order)}\n",
    "    \n",
    "    # A dict to store the returned data\n",
    "    data_dict = {\n",
    "        \"scaled_X_train\": scaled_X_train,\n",
    "        \"scaled_X_val\": scaled_X_val,\n",
    "        \"scaled_X_test\": scaled_X_test\n",
    "    }\n",
    "    \n",
    "    if output_var == \"tdir_down\":\n",
    "        Y_train_tdir_down = Y_train[:, :, output_col_indices[\"tdir_down\"]]\n",
    "        Y_val_tdir_down = Y_val[:, :, output_col_indices[\"tdir_down\"]]\n",
    "        Y_test_tdir_down = Y_test[:, :, output_col_indices[\"tdir_down\"]]\n",
    "        data_dict[\"Y_train_tdir_down\"] = Y_train_tdir_down\n",
    "        data_dict[\"Y_val_tdir_down\"] = Y_val_tdir_down\n",
    "        data_dict[\"Y_test_tdir_down\"] = Y_test_tdir_down\n",
    "        return data_dict\n",
    "            \n",
    "    elif output_var == \"tdif_down\":\n",
    "        Y_train_tdif_down = Y_train[:, :, output_col_indices[\"tdif_down\"]]\n",
    "        Y_val_tdif_down = Y_val[:, :, output_col_indices[\"tdif_down\"]]\n",
    "        Y_test_tdif_down = Y_test[:, :, output_col_indices[\"tdif_down\"]]\n",
    "        data_dict[\"Y_train_tdif_down\"] = Y_train_tdif_down\n",
    "        data_dict[\"Y_val_tdif_down\"] = Y_val_tdif_down\n",
    "        data_dict[\"Y_test_tdif_down\"] = Y_test_tdif_down\n",
    "        return data_dict\n",
    "        \n",
    "    elif output_var == \"tdir_up\":\n",
    "        Y_train_tdir_up = Y_train[:, :, output_col_indices[\"tdir_up\"]]\n",
    "        Y_val_tdir_up = Y_val[:, :, output_col_indices[\"tdir_up\"]]\n",
    "        Y_test_tdir_up = Y_test[:, :, output_col_indices[\"tdir_up\"]]\n",
    "        data_dict[\"Y_train_tdir_up\"] = Y_train_tdir_up\n",
    "        data_dict[\"Y_val_tdir_up\"] = Y_val_tdir_up\n",
    "        data_dict[\"Y_test_tdir_up\"] = Y_test_tdir_up\n",
    "        return data_dict\n",
    "        \n",
    "    elif output_var == \"tdif_up\":\n",
    "        Y_train_tdif_up = Y_train[:, :, output_col_indices[\"tdif_up\"]]\n",
    "        Y_val_tdif_up = Y_val[:, :, output_col_indices[\"tdif_up\"]]\n",
    "        Y_test_tdif_up = Y_test[:, :, output_col_indices[\"tdif_up\"]]\n",
    "        data_dict[\"Y_train_tdif_up\"] = Y_train_tdif_up\n",
    "        data_dict[\"Y_val_tdif_up\"] = Y_val_tdif_up\n",
    "        data_dict[\"Y_test_tdif_up\"] = Y_test_tdif_up\n",
    "        return data_dict\n",
    "        \n",
    "    elif output_var == \"spherical_albedo\":\n",
    "        Y_train_spherical_albedo = Y_train[:, :, output_col_indices[\"spherical_albedo\"]]\n",
    "        Y_val_spherical_albedo = Y_val[:, :, output_col_indices[\"spherical_albedo\"]]\n",
    "        Y_test_spherical_albedo = Y_test[:, :, output_col_indices[\"spherical_albedo\"]]\n",
    "        data_dict[\"Y_train_spherical_albedo\"] = Y_train_spherical_albedo\n",
    "        data_dict[\"Y_val_spherical_albedo\"] = Y_val_spherical_albedo\n",
    "        data_dict[\"Y_test_spherical_albedo\"] = Y_test_spherical_albedo\n",
    "        return data_dict\n",
    "        \n",
    "    elif output_var == \"edir\":\n",
    "        Y_train_edir = Y_train[:, :, output_col_indices[\"edir\"]]\n",
    "        Y_val_edir = Y_val[:, :, output_col_indices[\"edir\"]]\n",
    "        Y_test_edir = Y_test[:, :, output_col_indices[\"edir\"]]\n",
    "        data_dict[\"Y_train_edir\"] = Y_train_edir\n",
    "        data_dict[\"Y_val_edir\"] = Y_val_edir\n",
    "        data_dict[\"Y_test_edir\"] = Y_test_edir\n",
    "        return data_dict\n",
    "        \n",
    "    elif output_var == \"edif\":\n",
    "        Y_train_edif = Y_train[:, :, output_col_indices[\"edif\"]]\n",
    "        Y_val_edif = Y_val[:, :, output_col_indices[\"edif\"]]\n",
    "        Y_test_edif = Y_test[:, :, output_col_indices[\"edif\"]]\n",
    "        data_dict[\"Y_train_edif\"] = Y_train_edif\n",
    "        data_dict[\"Y_val_edif\"] = Y_val_edif\n",
    "        data_dict[\"Y_test_edif\"] = Y_test_edif\n",
    "        return data_dict\n",
    "        \n",
    "    elif output_var == \"path_rad\":\n",
    "        Y_train_path_rad = Y_train[:, :, output_col_indices[\"path_rad\"]]\n",
    "        Y_val_path_rad = Y_val[:, :, output_col_indices[\"path_rad\"]]\n",
    "        Y_test_path_rad = Y_test[:, :, output_col_indices[\"path_rad\"]]\n",
    "        data_dict[\"Y_train_path_rad\"] = Y_train_path_rad\n",
    "        data_dict[\"Y_val_path_rad\"] = Y_val_path_rad\n",
    "        data_dict[\"Y_test_path_rad\"] = Y_test_path_rad\n",
    "        return data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4ba1ef",
   "metadata": {},
   "source": [
    "<h3>Convert to .py script</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ed55a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook prepare_NN_data.ipynb to script\n",
      "[NbConvertApp] Writing 8261 bytes to prepare_NN_data.py\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to script prepare_NN_data.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cce98ee",
   "metadata": {},
   "source": [
    "- Jupyter notebooks need to be converted to python scripts ('.py' files) to be imported directly into another notebook / script.\n",
    "- Note that this notebook itself is not converted into a '.py' file, but rather a copy is made of this notebook as a '.py' file."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
